# Personalized LLM Decoding via Contrasting Personal Preference

## Introduction ## 

We present our new decoding framework for LLM personalization by Contrasting Personal preference (COPE). Our key idea is incorporating implicit reward signals for user preference to guide both training and inference

## Dataset ## 
We use publicly available data from the [LaMP](https://arxiv.org/abs/2304.11406) benchmark. You can download the our processed data [here](https://drive.google.com/file/d/1bJ3Rh_sqrw3suwwweFbra5CTV7GVjgxF/view?usp=sharing), unzip it, and place it under the ```./data``` folder

We utilize publicly available data from the [LaMP](https://arxiv.org/abs/2304.11406) benchmark. Our setting follows the [OPPU](https://arxiv.org/abs/2402.04401). You can download our preprocessed version [here](https://drive.google.com/file/d/147_uP-3A3XbEB8jwtaFkZXTXpLuybg8b/view?usp=sharing), extract the contents, and place them in the ./data directory.
